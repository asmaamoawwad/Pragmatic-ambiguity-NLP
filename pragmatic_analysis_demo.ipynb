{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Sarcasm Detection Demo\n",
                "\n",
                "This notebook illustrates how to use a **pre-trained AI model** to detect sarcasm in text. \n",
                "We will not train a model from scratch. Instead, we use a model that has already learned from millions of tweets.\n",
                "\n",
                "**Steps:**\n",
                "1. Load the pre-trained model.\n",
                "2. Run the model on example sentences.\n",
                "3. See how well it performs (Accuracy & Confusion Matrix)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Setup and Imports\n",
                "# We import the necessary libraries.\n",
                "# 'transformers' allows us to use the pre-trained model.\n",
                "# 'pandas' helps us manage data.\n",
                "# 'seaborn' and 'matplotlib' are for plotting the graph.\n",
                "\n",
                "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
                "from scipy.special import softmax\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.metrics import confusion_matrix, accuracy_score\n",
                "\n",
                "# This setup ensures graphs look nice\n",
                "sns.set_theme(style=\"whitegrid\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Load the Pre-trained Model\n",
                "# We are using a RoBERTa model trained specifically on tweets to detect irony/sarcasm.\n",
                "# Model Source: cardiffnlp/twitter-roberta-base-irony\n",
                "\n",
                "MODEL_NAME = \"cardiffnlp/twitter-roberta-base-irony\"\n",
                "\n",
                "print(\"Loading model... (this may take a moment)\")\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
                "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
                "print(\"Model loaded successfully!\")\n",
                "\n",
                "# Function to simplify prediction\n",
                "def predict_sarcasm_score(text):\n",
                "    # 1. Prepare text for the model\n",
                "    encoded_input = tokenizer(text, return_tensors='pt')\n",
                "    \n",
                "    # 2. Get the model's raw output\n",
                "    output = model(**encoded_input)\n",
                "    scores = output[0][0].detach().numpy()\n",
                "    \n",
                "    # 3. Convert scores to probabilities (0 to 1)\n",
                "    scores = softmax(scores)\n",
                "    \n",
                "    # The model returns [Probability of NOT Sarcastic, Probability of Sarcastic]\n",
                "    # We return the label with the highest probability\n",
                "    # 1 = Sarcastic (irony), 0 = Not Sarcastic\n",
                "    prediction_index = np.argmax(scores)\n",
                "    \n",
                "    label = \"Sarcastic\" if prediction_index == 1 else \"Literal\"\n",
                "    confidence = scores[prediction_index]\n",
                "    return label, confidence"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Run Inference on Examples\n",
                "# Here is a small list of examples to test the model (Gold Standard Data).\n",
                "\n",
                "examples = [\n",
                "    # Literal Statements (Should be 0/Literal)\n",
                "    {\"text\": \"I love my job, it is very rewarding.\", \"truth\": \"Literal\"},\n",
                "    {\"text\": \"The sky is blue today.\", \"truth\": \"Literal\"},\n",
                "    {\"text\": \"I missed the bus and now I am late.\", \"truth\": \"Literal\"},\n",
                "    \n",
                "    # Sarcastic Statements (Should be 1/Sarcastic)\n",
                "    {\"text\": \"Oh great, I missed the bus again. Just what I needed.\", \"truth\": \"Sarcastic\"},\n",
                "    {\"text\": \"I love working on weekends.\", \"truth\": \"Sarcastic\"},\n",
                "    {\"text\": \"Thanks for the help, you were completely useless.\", \"truth\": \"Sarcastic\"}\n",
                "]\n",
                "\n",
                "# We process each example\n",
                "results = []\n",
                "print(f\"{'Text':<50} | {'Prediction':<10} | {'Truth':<10}\")\n",
                "print(\"-\"*80)\n",
                "\n",
                "for item in examples:\n",
                "    text = item[\"text\"]\n",
                "    truth = item[\"truth\"]\n",
                "    \n",
                "    # Ask the AI\n",
                "    prediction, conf = predict_sarcasm_score(text)\n",
                "    \n",
                "    # Save result\n",
                "    results.append({\"text\": text, \"truth\": truth, \"prediction\": prediction})\n",
                "    \n",
                "    print(f\"{text[:47]+'...':<50} | {prediction:<10} | {truth:<10}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Evaluation Metrics\n",
                "# Let's see the overall accuracy and where mistakes happened.\n",
                "\n",
                "df_results = pd.DataFrame(results)\n",
                "\n",
                "# Compute Accuracy\n",
                "acc = accuracy_score(df_results['truth'], df_results['prediction'])\n",
                "print(f\"\\nOverall Model Accuracy: {acc*100:.2f}%\")\n",
                "\n",
                "# Create Confusion Matrix\n",
                "# This shows: How many Literal were correctly detected? How many Sarcastic were missed?\n",
                "labels = [\"Literal\", \"Sarcastic\"]\n",
                "cm = confusion_matrix(df_results['truth'], df_results['prediction'], labels=labels)\n",
                "\n",
                "# Plotting\n",
                "plt.figure(figsize=(6, 5))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
                "plt.xlabel('Predicted Label')\n",
                "plt.ylabel('True Label')\n",
                "plt.title('Confusion Matrix: Human vs AI')\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
